INFO:root:Namespace(model_name='ADRec', model_mode='')
INFO:root:--------------------------------------------- BEGIN: 2025-12-19 22:17:39 ---------------------------------------------
INFO:root:
===============================================
 Arguments              | Values               
===============================================
 batch_size             | 256                 
 ce_loss_weight         | 0.3                 
 cfg_dropout_rate       | 0.1                 
 cfg_scale              | 1.5                 
 data_appendix          |                     
 dataset                | Grocery_and_Gourm...
 diffusion_loss_weight  | 0.7                 
 diffusion_steps        | 50                  
 dropout                | 0                   
 early_stop             | 10                  
 emb_size               | 64                  
 epoch                  | 200                 
 eval_batch_size        | 256                 
 gpu                    | 0                   
 hidden_size            | 64                  
 history_max            | 20                  
 l2                     | 0                   
 lambda_uncertainty     | 0.001               
 lr                     | 0.001               
 main_metric            |                     
 noise_schedule         | linear              
 num_blocks             | 4                   
 num_heads              | 4                   
 num_neg                | 1                   
 num_workers            | 5                   
 optimizer              | Adam                
 random_seed            | 0                   
 rescale_timesteps      | False               
 save_final_results     | 1                   
 test_all               | 0                   
 topk                   | 5,10,20,50          
 use_partial_generation | 1                   
===============================================
INFO:root:Device: cpu
INFO:root:Load corpus from data/Grocery_and_Gourmet_Food\SeqReader.pkl
INFO:root:#params: 888064
INFO:root:ADRec(
  (projection): Identity()
  (emb_projection): Identity()
  (i_embeddings): Embedding(8714, 64)
  (embed_dropout): Dropout(p=0.2, inplace=False)
  (hist_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (diffusion): DiffusionModule(
    (denoise_net): DenoisedModel(
      (decoder): Sequential(
        (0): Linear(in_features=64, out_features=256, bias=True)
        (1): SiLU()
        (2): Linear(in_features=256, out_features=64, bias=True)
        (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (time_embed): Sequential(
        (0): Linear(in_features=64, out_features=256, bias=True)
        (1): SiLU()
        (2): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (condition_encoder): TransformerEncoder(
      (transformer): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
            )
            (linear1): Linear(in_features=64, out_features=256, bias=True)
            (dropout): Dropout(p=0, inplace=False)
            (linear2): Linear(in_features=256, out_features=64, bias=True)
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0, inplace=False)
            (dropout2): Dropout(p=0, inplace=False)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
            )
            (linear1): Linear(in_features=64, out_features=256, bias=True)
            (dropout): Dropout(p=0, inplace=False)
            (linear2): Linear(in_features=256, out_features=64, bias=True)
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0, inplace=False)
            (dropout2): Dropout(p=0, inplace=False)
          )
          (2): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
            )
            (linear1): Linear(in_features=64, out_features=256, bias=True)
            (dropout): Dropout(p=0, inplace=False)
            (linear2): Linear(in_features=256, out_features=64, bias=True)
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0, inplace=False)
            (dropout2): Dropout(p=0, inplace=False)
          )
          (3): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
            )
            (linear1): Linear(in_features=64, out_features=256, bias=True)
            (dropout): Dropout(p=0, inplace=False)
            (linear2): Linear(in_features=256, out_features=64, bias=True)
            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0, inplace=False)
            (dropout2): Dropout(p=0, inplace=False)
          )
        )
      )
      (position_embeddings): Embedding(1000, 64)
    )
  )
  (ce_loss): CrossEntropyLoss()
)
