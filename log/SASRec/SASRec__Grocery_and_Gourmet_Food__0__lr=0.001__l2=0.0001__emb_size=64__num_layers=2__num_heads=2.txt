INFO:root:Namespace(model_name='SASRec', model_mode='')
INFO:root:--------------------------------------------- BEGIN: 2025-12-02 15:12:46 ---------------------------------------------
INFO:root:
===========================================
 Arguments          | Values               
===========================================
 batch_size         | 256                 
 data_appendix      |                     
 dataset            | Grocery_and_Gourm...
 dropout            | 0                   
 early_stop         | 10                  
 emb_size           | 64                  
 epoch              | 200                 
 eval_batch_size    | 256                 
 gpu                | 0                   
 history_max        | 50                  
 l2                 | 0.0001              
 lr                 | 0.001               
 main_metric        |                     
 num_heads          | 2                   
 num_layers         | 2                   
 num_neg            | 1                   
 num_workers        | 5                   
 optimizer          | Adam                
 random_seed        | 0                   
 save_final_results | 1                   
 test_all           | 0                   
 topk               | 5,10,20,50          
===========================================
INFO:root:Device: cpu
INFO:root:Load corpus from data/Grocery_and_Gourmet_Food/SeqReader.pkl
INFO:root:#params: 603072
INFO:root:SASRec(
  (i_embeddings): Embedding(8714, 64)
  (p_embeddings): Embedding(51, 64)
  (transformer_block): ModuleList(
    (0): TransformerLayer(
      (masked_attn_head): MultiHeadAttention(
        (q_linear): Linear(in_features=64, out_features=64, bias=True)
        (k_linear): Linear(in_features=64, out_features=64, bias=True)
        (v_linear): Linear(in_features=64, out_features=64, bias=True)
      )
      (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0, inplace=False)
      (linear1): Linear(in_features=64, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (dropout2): Dropout(p=0, inplace=False)
    )
    (1): TransformerLayer(
      (masked_attn_head): MultiHeadAttention(
        (q_linear): Linear(in_features=64, out_features=64, bias=True)
        (k_linear): Linear(in_features=64, out_features=64, bias=True)
        (v_linear): Linear(in_features=64, out_features=64, bias=True)
      )
      (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0, inplace=False)
      (linear1): Linear(in_features=64, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=64, bias=True)
      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (dropout2): Dropout(p=0, inplace=False)
    )
  )
)
INFO:root:Test Before Training: (HR@5:0.0564,NDCG@5:0.0331,HR@10:0.1147,NDCG@10:0.0518,HR@20:0.2222,NDCG@20:0.0787,HR@50:0.5112,NDCG@50:0.1353)
INFO:root:Optimizer: Adam
INFO:root:Epoch 1     loss=0.4717 [57.4 s]	dev=(HR@5:0.3388,NDCG@5:0.2297) [12.1 s] *
INFO:root:Early stop manually
INFO:root:
--------------------------------------------- END: 2025-12-02 15:14:18 ---------------------------------------------
